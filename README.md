# ConvNext-Tiny with CBAM for Severstal Steel Defect Detection

Deep learning model for multi-label classification of steel surface defects using ConvNext-Tiny backbone enhanced with CBAM (Convolutional Block Attention Module) attention mechanisms.

## ğŸ¯ Features

- **ConvNext-Tiny + CBAM**: Modern CNN with spatial and channel attention at stages 3-4
- **Focal Loss**: Handles severe class imbalance with dynamic Î± computation
- **Full Resolution**: Processes 256Ã—1600 images without resizing to preserve small defects
- **Modular Architecture**: Registry pattern for easy model/loss swapping
- **Production Ready**: Type hints, logging, checkpointing, early stopping

## ğŸš€ Quick Start (Local)

```bash
# Install dependencies
pip install -r requirements.txt

# Validate system
python code/validate_system.py

# Train with default config
python code/train.py

# Or customize
python code/train.py data.batch_size=6 training.num_epochs=150
```

## â˜ï¸ RunPod Setup

See [GIT_LFS_SETUP.md](GIT_LFS_SETUP.md) for detailed GitHub/LFS setup instructions.

**Quick RunPod workflow:**

```bash
# 1. Clone on RunPod
git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
cd YOUR_REPO

# 2. Run setup script
chmod +x setup_runpod.sh
./setup_runpod.sh

# 3. Start training
python code/train.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ core/                 # Core modules
â”‚   â”‚   â”œâ”€â”€ models/          # ConvNext-Tiny + CBAM
â”‚   â”‚   â”œâ”€â”€ losses/          # Focal Loss, BCE
â”‚   â”‚   â”œâ”€â”€ data/            # Dataset, splitting
â”‚   â”‚   â”œâ”€â”€ augmentation/    # Image transforms
â”‚   â”‚   â””â”€â”€ training/        # Trainer, callbacks, metrics
â”‚   â”œâ”€â”€ train.py             # Training entry point
â”‚   â””â”€â”€ validate_system.py   # System validation
â”œâ”€â”€ config/
â”‚   â””â”€â”€ train_config.yaml    # Hydra configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ annotations/         # JSON annotations
â”‚   â”œâ”€â”€ zips/               # Data archives (Git LFS)
â”‚   â””â”€â”€ images/             # Extracted images (not in git)
â””â”€â”€ context/                 # Documentation
```

## ğŸ“Š Model Architecture

- **Backbone**: ConvNext-Tiny (28M parameters, ImageNet pretrained)
- **Attention**: CBAM modules at stages 3-4 (semantic level)
- **Head**: Dropout(0.5) â†’ Linear(num_features â†’ 4)
- **Output**: 4 logits for multi-label classification

## ğŸ”§ Configuration

All parameters in `config/train_config.yaml`:

```yaml
model:
  name: convnext_tiny_cbam
  cbam_stages: [3, 4]
  
loss:
  type: focal_loss
  gamma: 2.0
  
training:
  num_epochs: 100
  early_stopping_patience: 15
```

Override via command line:
```bash
python code/train.py loss.type=bce_with_logits data.batch_size=4
```

## ğŸ“š Documentation

- [QUICK_START.md](context/QUICK_START.md) - Common commands
- [ARCHITECTURE_README.md](context/ARCHITECTURE_README.md) - Design deep dive
- [PROJECT_INDEX.md](context/PROJECT_INDEX.md) - Complete file index
- [GIT_LFS_SETUP.md](GIT_LFS_SETUP.md) - GitHub & RunPod setup

## ğŸ“ Key Design Decisions

1. **CBAM at stages 3-4 only**: Optimal receptive field for small defects
2. **Focal Loss with dynamic Î±**: Auto-computed from class frequencies
3. **No image resizing**: Preserves spatial detail in 256Ã—1600 images
4. **Registry pattern**: UI-ready architecture for model/loss selection
5. **Stratified splitting**: Maintains class distribution in imbalanced data

## ğŸ“ˆ Expected Performance

Training on Severstal dataset (4 defect classes):
- **Class 1 (defect_1)**: ~85% F1
- **Class 2 (defect_2)**: ~15% F1 (rare class - 1.6%)
- **Class 3 (defect_3)**: ~95% F1 (common - 73%)
- **Class 4 (defect_4)**: ~78% F1
- **Macro F1**: ~89%

## ğŸ› ï¸ Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA-capable GPU (8GB+ VRAM recommended)
- See `requirements.txt` for full list

## ğŸ“ License

[Add your license here]

## ğŸ™ Acknowledgments

- Severstal dataset from Kaggle competition
- ConvNext architecture from Facebook Research
- CBAM attention module from "CBAM: Convolutional Block Attention Module" (Woo et al., 2018)
